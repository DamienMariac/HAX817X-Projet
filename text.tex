\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}


\title{Titre}
\author{Nom} 
\date{\today} 

\begin{document}

\maketitle 
\newpage
\section{Introduction} 

Le théorème central limite, formulé par Pierre-Simon de Laplace en 1809, garantit que, sous des conditions raisonnables, la somme normalisée de ces variables suit asymptotiquement une loi normale. Cette convergence est utile pour étudier le comportement \textbf{global} des observations, mais elle ne renseigne pas sur le comportement des valeurs extrêmes.

Il est donc naturel de se demander quelle peut être la convergence en loi de ses valeurs extrêmes. 
\\
Autrement dit, pour \( X = (X_1, ..., X_n) \) un échantillon de variables aléatoires i.i.d , on pose :


\[
M_n = \max\{X_i \mid i \in \{1, ..., n\} \}
\]

et on s'intéresse à la convergence de \( M_n \), ainsi qu'aux hypothèses sous lesquelles cette convergence a lieu.
\\
\\
\\
Remarque : Etudier le minimum est totalement analogue dans ce qui suit.

\section{Lois de $M_n$} 

Une premiere approche serait de considerer la fonction de repartition et d'utiliser le fait que les $X_i$ sont i.i.d :
\\
En effet,
\[
F_{M_n}(t) = \mathbb{P}(M_n < t) = \mathbb{P}(X_1 < t,...,X_n <t)=\mathbb{P}(X_1<t)^n = F_{X_1}^n(t) 
\]

Mais on rencontre un probleme ici, puisque si $n\to + \infty$, $F_{X_1}(t)^n$ converge vers 0 (ou 1 si t est la borne sup du support des $X_i$).
\\
\\
L'idée est donc d'introduire 2 suites ($b_n$) et ($a_n$) (avec $a_n > $  0 pour tout n) afin de pouvoir contrôler $M_n$.

Puis étudier la loi de la limite de $\frac{M_n - b_n}{a_n}$. Comme la fonction de repartition caracterise la loi, il nous suffit d'étudier la fonction $G$ définie pour tout t dans le support des $X_i$ comme :

\[
\mathbb{P} \left( \frac{M_n - b_n}{a_n} < t \right) \xrightarrow[n\to +\infty]{} G(t)
\]

à ce stade la, il nous faut donc trouver toutes les distributions G qui peuvent apparaître comme limite dans l’équation ci-dessus.
\\
\\
Pour ce faire, nous allons utiliser le théoreme suivant : 
\\
\\
\underline{\textbf{Théorème}:}
Soit \( Y_n \) une variable aléatoire de fonction de répartition \( F_n \), et soit \( Y \) une variable aléatoire de fonction de répartition \( F \).  
Alors $Y_n \xrightarrow{\mathcal{L} } Y$ si et seulement si pour toute fonction $z$ réelle, bornée et continue :
\[
\mathbb{E}[z(Y_n)] \to \mathbb{E}[z(Y)].
\]

En prenant ici $Y_n = \frac{M_n -b_n}{a_n}$, on obtient :
\[
\mathbb{E}[z(\frac{M_n -b_n}{a_n})] = \int_{-\infty}^{\infty} z(\frac{x-b_n}{a_n}) n F^{n-1} (x)dF(x)
\]

L'astuce ici va être de faire un changement de variable astucieux. On va poser : 

\[
x = Q(1-\frac{1}{y}) = K(y) \; \; \; \; \; \; \text{avec Q la fonction quantile}
\]

\[
\text{donc} \;\;\;\;\;\;\;\; \int_{-\infty}^{\infty} z(\frac{x-b_n}{a_n}) n F^{n-1} (x)dF(x) = \int_{0}^{n} z(\frac{K(\frac{n}{v}) - b_n}{a_n}) ( 1 - \frac{v}{n})^{n-1} dv
\]
\\
\\
Or, on a $\lim_{n \to \infty} ( 1 - \frac{v}{n})^{n-1} = e^{-v}$ , et on a $\lim_{n \to \infty} \int_{0}^{n} = \int_{0}^{+ \infty}$.
\\
On trouve ici une bonne valeur pour $b_n = K(n)$ (pas bien compris pourquoi).
\\
On obtient alors une condition, il faut qu'il existe une fonction $a$ tel que $\lim_{x \to \infty} \frac{K(\frac{x}{v}) - K(x)}{a(x)}$ converge (vers une fonction $h(n)$).
\\
\\
\textbf{Demontrons alors que :}
\\
Les limites possibles sont données par :

\[
ch_\gamma (u) = c \int_1^u v^{-\gamma - 1} dv = c \frac{u^\gamma - 1}{\gamma}.
\]

où \( c \geq 0 \) et \( \gamma \) est un réel.

Nous interpréterons \( h_0 (u) = \log u \) lorsque \( \gamma = 0 \).
\\
\\
\underline{preuve:}
Par hypothèse, on a :
\\
\[
\lim_{x \to \infty} \frac{K(xu) - K(x)}{a(x)} = h (u).
\]

Si nous supposons que la fonction d'échelle \( a(x) \) est de la forme :

\[
a(x) = x^\gamma \ell(x),
\]

où \( \ell(x) \) est une fonction de variation lente ($\text{i.e.} \; \forall a > 0 \; \; \lim_{x \to + \infty} \frac{l(ux)}{l(x)}=1$), alors la relation limite implique que :

\[
\frac{K(xu) - K(x)}{x^\gamma \ell(x)} \to h (u).
\]

En supposant que la fonction quantile de queue \( K(x) \) satisfait une loi de variation régulière, nous pouvons écrire :

\[
K(x) = x^\gamma \ell(x).
\]

Ainsi, nous avons :

\[
K(xu) - K(x) = (xu)^\gamma \ell(xu) - x^\gamma \ell(x).
\]

En divisant par \( x^\gamma \ell(x) \), nous obtenons :

\[
\frac{K(xu) - K(x)}{x^\gamma \ell(x)} = u^\gamma \frac{\ell(xu)}{\ell(x)} - 1.
\]

Comme \( \ell(x) \) est une fonction de variation lente, nous avons \( \ell(xu)/\ell(x) \to 1 \) lorsque \( x \to \infty \), ce qui donne :

\[
h (u) = u^\gamma - 1.
\]

Si nous introduisons un paramètre \( c \geq 0 \), nous pouvons écrire :

\[
ch_\gamma (u) = c \int_1^u v^{-\gamma - 1} dv.
\]

En intégrant :

\[
ch_\gamma (u) = c \frac{u^\gamma - 1}{\gamma}.
\]

Enfin, pour \( \gamma = 0 \), nous utilisons l'interprétation logarithmique de la variation lente :

\[
h_0 (u) = \log u.
\]

\hfill\(\square\)

Remarque : Etant données qu'on cherche une loi limite non dégénéré, on va exclure le cas où $c = 0$.
\section{Estimation du paramètre gamma} 

Soient \(X_1, \dots, X_n\) i.i.d. On définit la fonction de répartition :

\[
F(x) = P(X_1 \leq x), \quad x \in \mathbb{R}
\]

Nous allons définir la fonction de répartition empirique.

\subsection{Fonction de répartition empirique}

\textbf{Définition :} Pour tout \(x \in \mathbb{R}\), la fonction de répartition empirique est donnée par :

\[
F_n(x) = \frac{1}{n} \sum_{i=1}^{n} \mathbb{1}_{(-\infty, x]}(X_i)
\]

où \(\mathbb{1}_{(-\infty, x]}(X_i)\) est l'indicatrice de l'événement \(\{X_i \leq x\}\).
La fonction de répartition empirique utilise la statistique d'ordre.

\subsection{Quantiles et inverse généralisée}

\textbf{Définition :} Soit \(p \in (0,1)\), on appelle \textit{\(p\)-quantile}, noté \(x_p\), de la loi \(F\) toute quantité satisfaisant :

\[
F(x_p) = p
\]

Pour définir un quantile de manière plus générale, on considère l'inverse généralisée de \(F\).

\textbf{Définition :} Pour tout \(u \in [0,1]\), on appelle \textit{inverse généralisée} de \(F\), notée \(F^-\), la fonction définie par :

\[
F^-(u) = \inf \{ x \in \mathbb{R} \mid F(x) \geq u \}
\]

\subsection{Quantiles empiriques}

\textbf{Définition :} Soit \(p \in (0,1)\), on appelle \textit{\(p\)-quantile empirique}, noté \(x_p(n)\), la valeur :

\[
x_p(n) = F_n^-(p) = \inf \{ x \in \mathbb{R} \mid F_n(x) \geq p \}
\]

\subsection{Estimation de \(\gamma\) par la méthode des quantiles}

Pour déterminer les estimateurs de \(\gamma\) pour ces trois distributions, nous utilisons l'estimation par la méthode des quantiles.  
Nous allons commencer par estimer le paramètre \(\gamma\) pour la distribution de Fréchet.

\subsubsection{Rappel de la fonction de répartition}

Rappelons que la fonction de répartition de la loi de Fréchet s'écrit comme :

\[
F(x) = \exp(-x^{-\gamma}), \quad x > 0, \quad \gamma > 0.
\]

On cherche \(x\) tel que :

\[
\exp(-x^{-\gamma}) = p.
\]

En prenant le logarithme, on obtient :

\[
\ln(p) = -x^{-\gamma}
\]

\[
x^{\gamma} = -\frac{1}{\ln(p)}
\]

\subsubsection{Estimation de \(\gamma\) à partir du quantile médian}

On utilise l'estimateur du quantile avec la médiane de la distribution (\(x_{1/2}\)) :

\[
x_{1/2} = (\ln 2)^{-1/\gamma} = f(\gamma),
\]

où \(f(\gamma)\) est une fonction bijective.  
Si l'on sait estimer \(x_{1/2}\), alors on peut facilement estimer \(\gamma\) en inversant \(f\) :

\[
\gamma = f^{-1}(x_{1/2}).
\]

On résout :

\[
x = f(\gamma) \iff x = (\ln 2)^{-1/\gamma}
\]

\[
\iff (\ln 2)^{1/\gamma} = \frac{1}{x}
\]

\[
\iff \frac{1}{\gamma} \ln (\ln 2) = \ln \left(\frac{1}{x}\right) = -\ln x.
\]

D'où :

\[
\gamma = -\frac{\ln (\ln 2)}{\ln x} = f^{-1}(x).
\]

\subsubsection{Convergence et estimation empirique}

Rappelons que le quantile empirique converge en probabilité vers le quantile théorique :

\[
x_p(n) \xrightarrow{\mathbb{P}} x_p = F^{-1}(p).
\]

En particulier, pour \(p = \frac{1}{2}\) :

\[
x_{1/2}(n) \xrightarrow{\mathbb{P}} x_{1/2},
\]

où \(x_{1/2}(n)\) est le quantile empirique d'ordre \(1/2\) (médiane empirique).  

Or, la convergence en probabilité est stable par transformation continue, d'où :

\[
f^{-1}(x_{1/2}(n)) \xrightarrow{\mathbb{P}} f^{-1}(x_{1/2}) = \gamma.
\]

Ainsi, 

\[
\gamma = f^{-1}(x_{1/2}(n))
\]

est un estimateur convergent de \(\gamma\).  
Finalement, l'estimateur de \(\gamma\) est donné par :

\[
\hat{\gamma} = -\frac{\ln (\ln 2)}{\ln x_{1/2}(n)}.
\]
\subsection{Estimation de \(\gamma\) pour la distribution de Gumbel}

La fonction de répartition de Gumbel est donnée par :

\[
F(x) = \exp(-\exp(-x/\gamma)).
\]

Ainsi, pour \(p = \frac{1}{2}\), on a :

\[
F(x_{1/2}) = \frac{1}{2}
\]

\[
\iff \exp(-\exp(-x_{1/2}/\gamma)) = \frac{1}{2}
\]

\[
\iff -\exp(-x_{1/2}/\gamma) = -\ln(2)
\]

\[
\iff \exp(-x_{1/2}/\gamma) = \ln(2)
\]

\[
\iff -\frac{x_{1/2}}{\gamma} = \ln(\ln(2))
\]

\[
\iff \gamma = -\frac{x_{1/2}}{\ln(\ln(2))} = f(x_{1/2}),
\]

où \(f\) est une fonction continue.

\subsubsection{Convergence et estimation empirique}

En utilisant le quantile empirique, on a :

\[
x_{1/2}(n) \xrightarrow{\mathbb{P}} x_{1/2}.
\]

Puisque la convergence en probabilité est stable par transformation continue, il en résulte :

\[
f(x_{1/2}(n)) \xrightarrow{\mathbb{P}} \gamma = f(x_{1/2}).
\]

Ainsi, un estimateur convergent de \(\gamma\) est donné par :

\[
\hat{\gamma} = f(x_{1/2}(n)) = -\frac{x_{1/2}(n)}{\ln(\ln(2))}.
\]

\subsection{Estimation de \(\gamma\) pour la distribution de Weibull}

Ainsi, l'estimateur \(\hat{\gamma}\) est un estimateur convergent de \(\gamma\).

La fonction de répartition de Weibull est donnée par :

\[
F(x) = 1 - \exp(-x^{\gamma}).
\]

Pour \(p = \frac{1}{2}\), nous avons :

\[
F(x_{1/2}) = \frac{1}{2}
\]

\[
\iff 1 - \exp(-x_{1/2}^{\gamma}) = \frac{1}{2}
\]

\[
\iff \frac{1}{2} = \exp(-x_{1/2}^{\gamma})
\]

\[
\iff -\ln(2) = -x_{1/2}^{\gamma}
\]

\[
\iff \ln(2) = x_{1/2}^{\gamma}.
\]

En appliquant le logarithme népérien :

\[
\gamma \ln(x_{1/2}) = \ln(\ln 2)
\]

\[
\iff \gamma = \frac{\ln(\ln 2)}{\ln(x_{1/2})} = \varphi(x_{1/2}),
\]

où \(\varphi\) est une fonction continue.

\subsubsection{Convergence et estimation empirique}

Puisque \(x_{1/2}(n)\) converge en probabilité vers \(x_{1/2}\), on a :

\[
\varphi(x_{1/2}(n)) \xrightarrow{\mathbb{P}} \varphi(x_{1/2}) = \gamma.
\]

Finalement, un estimateur convergent de \(\gamma\) est donné par :

\[
\hat{\gamma} = \frac{\ln(\ln 2)}{\ln(x_{1/2}(n))}.
\]
\end{document}